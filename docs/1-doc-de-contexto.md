# üìÑ Proyecto: API de B√∫squeda y Chat sobre Documentos Normativos

## 1. üéØ Objetivo del Proyecto

Desarrollar una **API en FastAPI** que permita:

- Cargar documentos normativos.
- Indexarlos en un motor de b√∫squeda vectorial.
- Realizar b√∫squedas sem√°nticas en lenguaje natural.
- Mantener sesiones de chat (historial de preguntas y respuestas).

Todo el sistema deber√° ser **online** (por ahora), y deber√° ser **f√°cilmente portable a contenedores** mediante Docker.

---

## 2. üõ† Tecnolog√≠as utilizadas

| Tecnolog√≠a | Uso |
|:-----------|:----|
| FastAPI | Framework web para la API |
| Uvicorn | Servidor ASGI para correr FastAPI |
| PostgreSQL (en Docker) | Base de datos principal |
| SQLAlchemy | ORM para la conexi√≥n a PostgreSQL |
| psycopg2-binary | Driver de PostgreSQL |
| Docker | Contenerizaci√≥n de servicios |
| Docker Compose | Orquestar la API y la base de datos |
| OpenAI Embeddings o Sentence Transformers | Generaci√≥n de embeddings para b√∫squedas sem√°nticas |
| FAISS | Motor de b√∫squeda vectorial |
| Python-dotenv | Manejo de variables de entorno (.env) |

---

## 3. üìÅ Estructura de Carpetas del Proyecto

```plaintext
/backend
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # Arranca FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.py      # Endpoints de la API
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py     # M√≥dulo para manejar conversaciones (IA, l√≥gica de contexto)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py  # Carga y preprocesamiento de documentos
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat_session.py   # Modelos de SQLAlchemy para sesiones de chat
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py  # Conexi√≥n y gesti√≥n de base de datos PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ vector_store.py   # Utilidades para almacenar y consultar embeddings (FAISS)
‚îÇ
‚îú‚îÄ‚îÄ .env                 # Variables de entorno (nunca versionar directamente)
‚îú‚îÄ‚îÄ requirements.txt     # Librer√≠as necesarias del proyecto
‚îú‚îÄ‚îÄ Dockerfile           # Imagen de Docker para la API
‚îú‚îÄ‚îÄ docker-compose.yml   # Orquestaci√≥n de servicios Docker (API + PostgreSQL)
‚îî‚îÄ‚îÄ README.md            # Documentaci√≥n general del proyecto
```

---

## 4. üì¶ Contenedores Docker a utilizar

Inicialmente:
- **PostgreSQL** (contenedor para la base de datos)
- **API de FastAPI** (contenedor que levanta la aplicaci√≥n)

En el futuro se puede agregar:
- Un contenedor separado para FAISS (si se desea escalar el motor vectorial)

---

## 5. üåê Variables de entorno `.env`

Estas ser√°n las principales (ejemplo):

```bash
POSTGRES_USER=normativa_user
POSTGRES_PASSWORD=normativa_pass
POSTGRES_DB=normativa_db
POSTGRES_HOST=db
POSTGRES_PORT=5432

OPENAI_API_KEY=your-openai-api-key-here

# Otros par√°metros que necesitemos
```

---

## 6. üöÄ Librer√≠as confirmadas en `requirements.txt`

```plaintext
fastapi
uvicorn[standard]
psycopg2-binary
sqlalchemy
python-dotenv
openai
sentence-transformers
faiss-cpu
```

*Nota: Se puede ajustar si en el futuro decidimos usar solo OpenAI o solo Sentence Transformers.*

---

## 7. üìù Notas importantes

- Se trabajar√° **100% online** en esta primera versi√≥n.
- El contexto y estructura **NO deben cambiar** una vez definidos.
- El motor de embeddings puede ser **OpenAI o Sentence Transformers**, seg√∫n facilidad y velocidad de implementaci√≥n para la demo.
- **FAISS** ser√° el motor vectorial base para almacenar y buscar embeddings.
- **PostgreSQL** ser√° nuestra base de datos principal (manejado en contenedor Docker).

---

# üìç Resumen Final

**Esta es la √∫nica estructura, tecnolog√≠as y enfoque que seguiremos hasta terminar esta primera versi√≥n.**  
Todo cambio posterior debe ser previamente discutido.

---

# **Visi√≥n General de la Arquitectura**

Tu objetivo ser√≠a construir un sistema con estas *grandes piezas*:

1. **Servidor de orquestaci√≥n (backend principal)**
   - Ser√° quien maneje: peticiones del cliente m√≥vil, conexi√≥n a LLM (OpenAI), contexto, historial, etc.
   - Aqu√≠ utilizar√°s **LangChain** para orquestar (manejar) toda la l√≥gica.

2. **Motor de recuperaci√≥n de informaci√≥n (RAG)**
   - Sistema que busca informaci√≥n precisa en bases de datos vectoriales (por ejemplo, FAISS, Chroma, o Pinecone).
   - Cuando llega una consulta, en lugar de preguntarle todo a OpenAI directamente, primero recuperas fragmentos relevantes de tus documentos cargados, y luego se los pasas al LLM para que responda.

3. **Base de datos de vectores (Vector Store)**
   - Donde almacenar√°s tus documentos procesados como vectores.
   - Aqu√≠ es donde ir√°n las leyes de tr√°nsito, manuales, documentos, etc.
   - Puede ser local (FAISS, Chroma) o en la nube (Pinecone, Weaviate).

4. **Manejador de contexto e historial de conversaci√≥n**
   - Para que tu asistente sepa de qu√© se habl√≥ antes.
   - Puedes usar herramientas como **LangChain Memory** (`ConversationBufferMemory`, `ConversationSummaryMemory`, etc.).
   - Esto permitir√° mantener el "hilo" de la conversaci√≥n.

5. **Sistema de sin√≥nimos / Diccionario personalizado**
   - Necesitas interceptar o preprocesar las consultas del usuario para reemplazar o mapear t√©rminos.
   - Se puede hacer como:
     - Preprocesamiento del input del usuario antes de mandarlo al LLM.
     - O enriquecer tu base de datos de vectores agregando las variantes sem√°nticas.

6. **Conexi√≥n a LLM**
   - Te conectar√°s a OpenAI, Anthropic, u otro modelo v√≠a API.
   - Pero tu servidor es el que controla qu√© le mandas al modelo, incluyendo los documentos encontrados + contexto de conversaci√≥n.

7. **Aplicaci√≥n m√≥vil (Frontend)**
   - Que consume tu API/servidor.
   - Aqu√≠ te puedes despreocupar un rato, lo principal ahora es levantar el servidor.

---

# **Flujo de Operaci√≥n (Simplificado)**

1. El usuario env√≠a una consulta por la app m√≥vil.
2. El servidor la recibe.
3. Se aplica normalizaci√≥n: diccionario de sin√≥nimos.
4. El servidor consulta el **vector store** para recuperar documentos relevantes (RAG).
5. Combina los documentos + historial de conversaci√≥n.
6. Env√≠a todo eso al **LLM** (OpenAI API).
7. Recibe la respuesta del modelo.
8. Se guarda el nuevo fragmento en el historial/contexto.
9. Se devuelve la respuesta al usuario.

---

# **Tecnolog√≠as y Herramientas que podr√≠as usar**

| Componente | Opci√≥n recomendada |
|------------|--------------------|
| **Framework Backend** | FastAPI (Python) |
| **Orquestador LLM** | LangChain |
| **Vector Store** | Chroma o FAISS (local) |
| **Base de Datos "normal"** | PostgreSQL para usuarios, historial |
| **Memory Manager** | LangChain Memory |
| **Motor de sin√≥nimos** | Diccionario interno + fuzzy matching √≥ OpenAI Embeddings |
| **Modelo LLM** | OpenAI (gpt-3.5, gpt-4) |

---

# **Diagrama r√°pido (mental)**

```
App M√≥vil ---> API Servidor
               |
          [LangChain]
               |
  +------------------------------+
  |  Contexto Memoria Historial   |
  |  Recuperador Vectorial (RAG)  |
  |  Sin√≥nimos / Normalizador     |
  |  Conexi√≥n LLM (OpenAI API)    |
  +------------------------------+
```

---
